[![DOI](https://zenodo.org/badge/719007511.svg)](https://zenodo.org/doi/10.5281/zenodo.10134440)

![logo](/images/OPLITE_logo_text.png)

# OPLITE - Vahvistusoppiminen

## Vahvistusoppiminen

Vahvistusoppiminen (engl. reinforcement learning) on koneoppimisen ongelmanratkaisutekniikka, jossa agentti tutkii ymp√§rist√∂√§, jonka tilan se havaitsee, ja toimii sen mukaisesti. Ymp√§rist√∂ antaa toiminnan mukaan palautetta, joka voi olla positiivista tai negatiivista. Algoritmi pyrkii l√∂yt√§m√§√§n ongelmaan ratkaisun, joka tuottaa eniten positiivista palautetta. L√§hde: [https://fi.wikipedia.org/wiki/Vahvistusoppiminen](https://fi.wikipedia.org/wiki/Vahvistusoppiminen).

## Reittihaku-sovellus

T√§m√§ sovellus k√§ytt√§√§ vahvistusoppimista l√∂yt√§√§kseen simulaatiomallissa liikkuvalle mobiilirobotille reitin l√§pi alueen, jolle k√§ytt√§j√§ voi asettaa esteit√§ ja siirt√§√§ niiden paikkaa.

Sovelluksen ja simulaatiomallin toiminta on esitetty alla olevassa sekvenssikaaviossa.

```mermaid
sequenceDiagram
    participant Simulaatiomalli
    participant Reittihaku-sovellus
    Simulaatiomalli->>Simulaatiomalli: Karttadatan muodostaminen
    Simulaatiomalli->>Reittihaku-sovellus: Karttadatan l√§hetys
    Reittihaku-sovellus->>Reittihaku-sovellus: Reitin haku
    Reittihaku-sovellus->>Reittihaku-sovellus: Toimintojen muodostus
    Reittihaku-sovellus->>Simulaatiomalli: Toimintojen (reitin) l√§hetys
    Simulaatiomalli->>Simulaatiomalli: Mobiilirobotin ajo pitkin reitti√§
```

## Simulaatiomallit

Reittihaku-sovellus on suunniteltu toimimaan yhdess√§ seuraavien simulaatiomallien kanssa:
* OPLITE_Reinforcement_Learning_external.vcmx (Visual Components)
* oplite_reinforcement_learning_external.spp (Plant Simulation)

Alla olevat simulaatiomallit toimivat itsen√§isesti ilman ulkoista reittihaku-sovellusta. Niit√§ voi k√§ytt√§√§, jos tietokoneelle ei ole asennettu Python-ohjelmointikielt√§ tai jos simulaatiomallin ja ulkoisen sovelluksen v√§lisess√§ yhteydess√§ on jotain ongelmaa.
* OPLITE_Reinforcement_Learning_python2.vcmx (Visual Components)
* oplite_reinforcement_learning_simtalk.spp (Plant Simulation)

Kaikki simulaatiomallit l√∂ytyv√§t *sims*-kansiosta.

Huomaa, ett√§ Plant Simulation ja Visual Components vaativat maksullisen lisenssin toimiakseen.

## Reittihaku-sovelluksen k√§ytt√∂

K√§ynnist√§ reittihaku-sovellus kirjoittamalla alla oleva teksti komentokehotteeseen:

```python
python app.py
```

Avaa sitten simulaatiomalli, aseta esteet ja k√§ynnist√§ simulaatio. Kun reitti on l√∂ytynyt, mobiilirobotti liikkuu simulaatiomallissa reittihaun algoritmin l√∂yt√§m√§n reitin mukaisesti.

Jos haluat vaihtaa esteiden paikkaa, resetoi simulaatio, aseta esteiden sijainti ja k√§ynnist√§ lopuksi simulaatio uudestaan.

## Sovelluksen toiminta

### Frozen Lake -ymp√§rist√∂

Sovellus hy√∂dynt√§√§ reittihaussa *gymnasium*-paketin *Frozen Lake* -ymp√§rist√∂√§:

[https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

Kyseisess√§ ymp√§rist√∂ss√§ agentille on m√§√§ritetty nelj√§ mahdollista toimintoa:
* 0: liike vasemmalle
* 1: liike alas
* 2: liike oikealle
* 3: liike yl√∂s

Palkkio on *Frozen Lake*-ymp√§rist√∂ss√§ 1, jos agentti saavuttaa reitin loppupisteen, muussa tapauksessa 0.

### Karttadatan muodostaminen

Kaikissa simulaatiomalleissa on neli√∂m√§inen alue, jolla mobiilirobotti liikkuu. K√§ytt√§j√§ voi asettaa t√§lle alueelle esteit√§, joita mobiilirobotin tulee v√§ist√§√§ matkallaan l√§ht√∂pisteest√§ loppupisteeseen. Mobiilirobotin kulkeman reitin l√§ht√∂piste on alueen vasemmassa yl√§nurkassa, kun taas loppupiste on alueen oikeassa alanurkassa.

Alueen karttaa kuvataan merkkijonolla, joka koostuu kirjaimista *S*, *F*, *H* ja *G* siten, ett√§
* *S* (start) on reitin alkupiste
* *F* (frozen) on piste, jossa ei ole estett√§
* *H* (hole) on piste, jossa on este
* *G* (goal) on reitin loppupiste

Kirjaimet vastaavat Frozen Lake -ymp√§rist√∂n merkint√§tapaa.

Merkkijono muodostetaan alueen pisteist√§ l√§htem√§ll√§ liikkeelle reitin alkupisteest√§ eli vasemmasta yl√§nurkasta, josta liikutaan alasp√§in rivi kerrallaan. Kukin rivi luetaan vasemmalta oikelle.

Esimerkiksi merkkijono

"SFHHFFFFFFFFFHFFFFFFFHFFFFFFFHFFFHHHFHFFFFFFFHFFFFHFFFFFFFHFFFFG"

kuvaa alla olevaa 8x8-aluetta.

![karttaesimerkki](/images/karttaesimerkki.png)

**Kuvio 1.** Karttaesimerkki.

### Karttadatan l√§hetys

Simulaatiomalli l√§hett√§√§ karttadatan, alueen koon ja simulaatio-ohjelman lyhenteen (ps = Plant Simulation, vc = Visual Components) reittihaku-sovelluksessa olevalle palvelin-pistokkeelle. Alla on esimerkki Plant Simulation -ohjelman l√§hett√§m√§st√§ viestist√§.
```
{"mapdata": "SFFFFHHFFHFFFFFG", "mapsize": 4, "simsoft": "ps"}
```

### Reittihaun algoritmi

Reittihaun algoritmi koostuu kahdesta vaiheesta. Aluksi p√§ivitet√§√§n Q-taulu, jonka avulla saadaan selville ne toiminnot, joiden avulla mobiilirobotti p√§√§se alkupisteest√§ loppupisteeseen kulkien pitkin algoritmin l√∂yt√§m√§√§ reitti√§.

Q-taulun p√§ivityksen pseudokoodi:
![pseudokoodi](/images/pseudokoodi_q_taulu.png)

Reitin toimintojen m√§√§rityksen pseudokoodi:
![pseudokoodi](/images/pseudokoodi_toiminnot.png)

Kuviossa 2 on esitetty vihre√§ll√§ taustav√§rill√§ er√§s ratkaisu, jolla agentti p√§√§see alkupisteest√§ loppupisteeseen.

![ratkaisu](/images/ratkaisu.png)

**Kuvio 2.** Ratkaisu.

Ratkaisu annetaan toimintojen listana, joka on t√§ss√§ esimerkkitapauksessa:

[1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2]

### Q-taulu

Reittihaun algoritmissa k√§ytet√§√§n Q-taulua, jossa pidet√§√§n kirjaa kunkin tilan eri toiminnoista saaduista palkkioista. Q-taulu on k√§yt√§nn√∂ss√§ matriisi, jonka rivien lukum√§√§r√§ vastaa tilojen lukum√§√§r√§√§ ja sarakkeiden lukum√§√§r√§ vastaa toimintojen lukum√§√§r√§√§.

Q-taulu alustetaan nollaksi ennen algoritmin ajoa (taulukko 1).

**Taulukko 1.** Q-taulu.

![qtaulu](/images/qtaulu.png)

Q-taulu p√§ivitet√§√§n alla olevan yht√§l√∂n mukaisesti:

![qupdate](/images/qupdate.png)

miss√§
* *s* on tila (engl. state)
* *a* on toiminto (engl. action)
* *t* on ajan hetki (engl. time)
* *Œ±* on oppimisnopeus (engl. learning rate)
* *r* on palkkio (engl. reward)
* *Œ≥* on alennustekij√§ (engl. discount factor)

Oppimisnopeus (0 < ùõº ‚â§ 1) m√§√§ritt√§√§ miss√§ m√§√§rin uusi tieto peittoaa vanhan tiedon. Jos oppimisnopeus on 0, agentti ei opi mit√§√§n. Jos taas oppimisnopeus on 1, agentti hylk√§√§ vanhan tiedon ja k√§ytt√§√§ vain uutta tietoa. Oppimisnopeudelle k√§ytet√§√§n usein jotain vakioarvoa.

Alennustekij√§ *Œ≥* m√§√§ritt√§√§ miten t√§rke√§ tuleva palkkio on. Arvo 0 saa agentin tavoittelemaan vain nykyist√§ palkkiota, kun taas alennustekij√§, joka l√§hestyy arvoa 1, saa agentin tavoittelemaan pitk√§n aikav√§lin palkkiota.

### Reitin l√§hetys

Sovellus l√§hett√§√§ algoritmin l√∂yt√§m√§n ratkaisun simulaatio-ohjelmalle. Viesti koostuu toiminnoista, jotka on erotettu pilkulla toisistaan.

### Reitin visualisointi

Lopuksi simulaatio-ohjelma visualisoi mobiilirobotin reitin, kun toiminnot on vastaanotettu reittihaku-sovellukselta.

## Tekij√§tiedot

Hannu Hakalahti, Asiantuntija TKI, Sein√§joen ammattikorkeakoulu

## Hanketiedot

* Hankkeen nimi: OPLITE - Optimaalista lis√§arvoa teknologiasta
* Rahoittaja: Etel√§-Pohjanmaan liitto
* Aikataulu: 1.1.2023 - 31.12.2025
* Hankkeen kotisivut: [https://projektit.seamk.fi/alykkaat-teknologiat/oplite/](https://projektit.seamk.fi/alykkaat-teknologiat/oplite/)
---
![EU_logo](/images/Euroopan_unionin_osarahoittama_POS.png)
![EPLiitto](/images/EPLiitto_logo_vaaka_vari.jpg)
![SeAMK](/images/SEAMK_vaaka_fi_en_RGB_1200x486.jpg)
